{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13kzH_7yPFH4Fe8ri6mOmtYNr7xj6RIB4","authorship_tag":"ABX9TyNbMEtTsfjheFFJvMN2F0J+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxkWAfRJek3g","executionInfo":{"status":"ok","timestamp":1694154749329,"user_tz":-360,"elapsed":134136,"user":{"displayName":"thesis research","userId":"06634674941398765871"}},"outputId":"f04c7713-8805-4ac9-829a-9fd9804647a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Green percentage calculation complete.\n","New results saved to /content/drive/MyDrive/imgpath_gvi.csv\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","# Define the input folder where preprocessed images are located\n","input_folder = \"/content/drive/MyDrive/preprocessing_multiprocessingmodule\"\n","\n","# Define the output CSV file path for storing results\n","result_csv_path = \"/content/drive/MyDrive/imgpath_gvi.csv\"\n","\n","def calculate_green_percentage(image_path):\n","    image = cv2.imread(image_path)\n","    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    lower_green = np.array([40, 40, 40])\n","    upper_green = np.array([80, 255, 255])\n","    green_mask = cv2.inRange(hsv_image, lower_green, upper_green)\n","\n","    # Calculate the green percentage with two decimal places\n","    total_pixels = image.shape[0] * image.shape[1]\n","    green_pixels = np.sum(green_mask > 0)\n","    green_percentage = round((green_pixels / total_pixels) , 4)\n","\n","    return image_path, green_percentage  # Return the full image path\n","\n","# Check if the result CSV file exists and remove it\n","if os.path.exists(result_csv_path):\n","    os.remove(result_csv_path)\n","\n","# List all preprocessed image files\n","image_files = [os.path.join(input_folder, filename) for filename in os.listdir(input_folder) if filename.endswith(\".jpg\")]\n","\n","# Calculate green percentage for each preprocessed image\n","results = []\n","for image_path in image_files:\n","    image_path, green_percentage = calculate_green_percentage(image_path)\n","    results.append([image_path, green_percentage])  # Include full image path\n","\n","# Save the results to a new CSV file\n","df = pd.DataFrame(results, columns=['Image', 'Green Percentage'])  # Update column name\n","df.to_csv(result_csv_path, index=False)\n","\n","print(\"Green percentage calculation complete.\")\n","print(f\"New results saved to {result_csv_path}\")\n"]},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from sklearn.model_selection import train_test_split\n","\n","# Define the input CSV file containing image paths and green percentages\n","csv_file_path = \"/content/drive/MyDrive/imgpath_gvi.csv\"\n","\n","# Load the CSV file into a DataFrame\n","df = pd.read_csv(csv_file_path)\n","\n","# Define the input folder where images are located\n","input_folder = \"/content/drive/MyDrive/preprocessing_multiprocessingmodule\"\n","\n","# Load and preprocess images while resizing to a consistent size (e.g., 224x224)\n","def load_and_preprocess_image(image_path):\n","    img = cv2.imread(os.path.join(input_folder, image_path))\n","    img = cv2.resize(img, (224, 224))\n","    img = img / 255.0  # Normalize pixel values to [0, 1]\n","    return img\n","\n","# Define a generator to load and preprocess images in batches\n","def image_data_generator(df, batch_size):\n","    while True:\n","        for i in range(0, len(df), batch_size):\n","            batch_df = df.iloc[i:i+batch_size]\n","            batch_images = [load_and_preprocess_image(image_path) for image_path in batch_df['Image']]\n","            batch_X = np.array(batch_images)\n","            batch_y = batch_df['Green Percentage'].values\n","            yield batch_X, batch_y\n","\n","\n"],"metadata":{"id":"6WjA0DgUiCVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(df['Image'], df['Green Percentage'], test_size=0.2, random_state=42)\n","\n","# Create the CNN model\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n","    MaxPooling2D((2, 2)),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='linear')  # Linear activation for regression\n","])\n"],"metadata":{"id":"gpllt4z-iJca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n","\n"],"metadata":{"id":"uV5I3lM5iOqn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define batch size\n","batch_size = 32\n","\n","# Train the model using the generator\n","steps_per_epoch = len(X_train) // batch_size\n","validation_steps = len(X_test) // batch_size\n","epochs = 10\n","\n"],"metadata":{"id":"4Ruh-2RliRr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_generator = image_data_generator(pd.DataFrame({'Image': X_train, 'Green Percentage': y_train}), batch_size)\n","test_generator = image_data_generator(pd.DataFrame({'Image': X_test, 'Green Percentage': y_test}), batch_size)\n","\n"],"metadata":{"id":"Abdtd33ViUTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=epochs,\n","                    validation_data=test_generator, validation_steps=validation_steps)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ey9wIXk9iWQQ","executionInfo":{"status":"ok","timestamp":1694159802690,"user_tz":-360,"elapsed":4298587,"user":{"displayName":"thesis research","userId":"06634674941398765871"}},"outputId":"a328f580-13de-411f-fd0f-e47b68d1035b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","127/127 [==============================] - 417s 3s/step - loss: 0.7391 - mae: 0.3616 - val_loss: 0.1506 - val_mae: 0.2826\n","Epoch 2/10\n","127/127 [==============================] - 440s 3s/step - loss: 0.1392 - mae: 0.2786 - val_loss: 0.1242 - val_mae: 0.2664\n","Epoch 3/10\n","127/127 [==============================] - 434s 3s/step - loss: 0.1147 - mae: 0.2627 - val_loss: 0.1028 - val_mae: 0.2511\n","Epoch 4/10\n","127/127 [==============================] - 422s 3s/step - loss: 0.0938 - mae: 0.2364 - val_loss: 0.0818 - val_mae: 0.2194\n","Epoch 5/10\n","127/127 [==============================] - 424s 3s/step - loss: 0.0788 - mae: 0.2148 - val_loss: 0.0677 - val_mae: 0.2031\n","Epoch 6/10\n","127/127 [==============================] - 440s 3s/step - loss: 0.0688 - mae: 0.2017 - val_loss: 0.0586 - val_mae: 0.1952\n","Epoch 7/10\n","127/127 [==============================] - 440s 3s/step - loss: 0.0639 - mae: 0.1960 - val_loss: 0.0527 - val_mae: 0.1892\n","Epoch 8/10\n","127/127 [==============================] - 416s 3s/step - loss: 0.0602 - mae: 0.1894 - val_loss: 0.0489 - val_mae: 0.1853\n","Epoch 9/10\n","127/127 [==============================] - 438s 3s/step - loss: 0.0594 - mae: 0.1891 - val_loss: 0.0469 - val_mae: 0.1841\n","Epoch 10/10\n","127/127 [==============================] - 428s 3s/step - loss: 0.0596 - mae: 0.1890 - val_loss: 0.0453 - val_mae: 0.1812\n"]}]},{"cell_type":"code","source":["loss, mae = model.evaluate(test_generator, steps=validation_steps)\n","print(f\"Mean Absolute Error (MAE): {mae}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHYV51w7zH-l","executionInfo":{"status":"ok","timestamp":1694159943399,"user_tz":-360,"elapsed":41688,"user":{"displayName":"thesis research","userId":"06634674941398765871"}},"outputId":"915ea649-fec5-4c0e-b36b-75ce0c299915"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 29s 919ms/step - loss: 0.0453 - mae: 0.1812\n","Mean Absolute Error (MAE): 0.18120834231376648\n"]}]},{"cell_type":"code","source":["# Specify the path to save the trained model\n","model_save_path = \"/content/drive/MyDrive/cnn_trained_model.h5\"\n","\n","# Save the trained model\n","model.save(model_save_path)\n"],"metadata":{"id":"4xTmlc1g0LPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Define the lower and upper HSV thresholds for green\n","lower_green = np.array([40, 40, 40])\n","upper_green = np.array([80, 255, 255])\n","\n","# Define a function to preprocess the new image\n","def preprocess_image(image_path):\n","    # Load the image\n","    image = cv2.imread(image_path)\n","\n","    # Convert the image to HSV color space\n","    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","\n","    # Create a mask for the green regions\n","    green_mask = cv2.inRange(hsv_image, lower_green, upper_green)\n","\n","    # Calculate the green percentage\n","    total_pixels = image.shape[0] * image.shape[1]\n","    green_pixels = np.sum(green_mask > 0)\n","    green_percentage = (green_pixels / total_pixels) * 100.0\n","\n","    return green_percentage\n"],"metadata":{"id":"IR5JozS217Vt","executionInfo":{"status":"ok","timestamp":1694496549326,"user_tz":-360,"elapsed":1015,"user":{"displayName":"thesis research","userId":"06634674941398765871"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Define the path to the new image you want to analyze\n","new_image_path = \"/content/drive/MyDrive/images/3484_sat_34.jpg\"\n","\n","# Calculate the green percentage of the new image\n","predicted_green_percentage = preprocess_image(new_image_path)\n","\n","# Print the predicted green percentage\n","print(f\"Detected Green View Index in your image is: {predicted_green_percentage:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnNHiJzU1-dT","executionInfo":{"status":"ok","timestamp":1694160770768,"user_tz":-360,"elapsed":523,"user":{"displayName":"thesis research","userId":"06634674941398765871"}},"outputId":"a5a2ab07-e145-4bcb-8513-e224bdb2a501"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected Green View Index in your image is: 54.94%\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCjahnU8215s","executionInfo":{"status":"ok","timestamp":1694496449421,"user_tz":-360,"elapsed":19759,"user":{"displayName":"thesis research","userId":"06634674941398765871"}},"outputId":"f471d2c4-12ae-49f1-e785-b47f77a8b61c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]}]}